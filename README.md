# Model-Quantization


## Large Language Model (LLM)

### 2023

<!-- * [![Publish](https://img.shields.io/badge/<leaf tag>-<right tag>-<color>)]() 
[![Star](shields.io_url)](github_url) 
[paper title](paper url). 
some author. 
[[Paper]](paper url)
 [[Github]](github url)-->


* [![Publish](https://img.shields.io/badge/Conference-ICLR'22-blue)]() [![Star](https://img.shields.io/github/stars/IST-DASLab/gptq.svg?style=social&label=Star)](https://github.com/IST-DASLab/gptq) [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323). Elias Frantar, Saleh Ashkboos, Torsten Hoefler, Dan Alistarh. [[Paper]](https://arxiv.org/abs/2210.17323) [[Github]](https://github.com/IST-DASLab/gptq)

* [![Star](https://img.shields.io/github/stars/qwopqwop200/GPTQ-for-LLaMa.svg?style=social&label=Star)](https://github.com/qwopqwop200/GPTQ-for-LLaMa) [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa): 4 bits quantization of LLaMA using GPTQ. [[Github]](https://github.com/qwopqwop200/GPTQ-for-LLaMa)

* [![Publish](https://img.shields.io/badge/Conference-ICML'23-blue)]() [![Star](https://img.shields.io/github/stars/mit-han-lab/smoothquant.svg?style=social&label=Star)](https://github.com/mit-han-lab/smoothquant) [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438). Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han. [[Paper]](https://arxiv.org/abs/2211.10438) [[Github]](https://github.com/mit-han-lab/smoothquant)

* [![Star](https://img.shields.io/github/stars/mit-han-lab/llm-awq.svg?style=social&label=Star)](https://github.com/mit-han-lab/llm-awq) [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978). Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han. [[Paper]](https://github.com/mit-han-lab/llm-awq) [[Github]](https://github.com/mit-han-lab/llm-awq)

* [![Star](https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM.svg?style=social&label=Star)](https://github.com/hahnyuan/RPTQ4LLM) [RPTQ: Reorder-based Post-training Quantization for Large Language Models](https://arxiv.org/abs/2304.01089). Zhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe Wu. [[Paper]](https://arxiv.org/abs/2304.01089) [[Github]](https://github.com/hahnyuan/RPTQ4LLM)

* [![Star](https://img.shields.io/github/stars/artidoro/qlora.svg?style=social&label=Star)](https://github.com/artidoro/qlora) [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer. [[Paper]](https://arxiv.org/abs/2305.14314) [[Github]](https://github.com/artidoro/qlora)

* [ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](https://arxiv.org/abs/2303.08302). Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. [[Paper]](https://arxiv.org/abs/2303.08302)

* [![Star](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM.svg?style=social&label=Star)](https://github.com/SqueezeAILab/SqueezeLLM) [SqueezeLLM: Dense-and-Sparse Quantization](https://arxiv.org/pdf/2306.07629.pdf). Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W. Mahoney, Kurt Keutzer. [[Paper]](https://arxiv.org/pdf/2306.07629.pdf) [[Github]](https://github.com/SqueezeAILab/SqueezeLLM)

* [Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling](https://arxiv.org/abs/2304.09145v1). Xiuying Wei , Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu. [[Paper]](https://arxiv.org/abs/2304.09145v1)

* [Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models](https://arxiv.org/abs/2305.12356). Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang, Mao Yang, Shanghang Zhang, Ningyi Xu. [[Paper]](https://arxiv.org/abs/2305.12356)

* [LLM-QAT: Data-Free Quantization Aware Training for Large Language Models](https://arxiv.org/abs/2305.17888). 
Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, Vikas Chandra. [[Paper]](https://arxiv.org/abs/2305.17888)

* [![Star](https://img.shields.io/github/stars/Vahe1994/SpQR.svg?style=social&label=Star)](https://github.com/Vahe1994/SpQR) [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/abs/2306.03078). Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh. [[Paper]](https://arxiv.org/abs/2306.03078)  [[Github]](https://github.com/Vahe1994/SpQR)

* [![Star](https://img.shields.io/github/stars/xvyaward/owq.svg?style=social&label=Star)](https://github.com/xvyaward/owq) [OWQ: Lessons learned from activation outliers for weight quantization in large language models](https://arxiv.org/abs/2306.02272). Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park. [[Paper]](https://arxiv.org/abs/2306.02272) [[Github]](https://github.com/xvyaward/owq)

* [![Star](https://img.shields.io/github/stars/RUCAIBox/QuantizedEmpirical.svg?style=social&label=Star)](https://github.com/RUCAIBox/QuantizedEmpirical) [Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study](https://arxiv.org/abs/2307.08072). Peiyu Liu, Zikang Liu, Ze-Feng Gao, Dawei Gao, Wayne Xin Zhao, Yaliang Li, Bolin Ding, Ji-Rong Wen. [[Paper]](https://arxiv.org/abs/2307.08072) [[Github]](https://github.com/RUCAIBox/QuantizedEmpirical) 

* [ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats](https://arxiv.org/abs/2307.09782). Xiaoxia Wu, Zhewei Yao, Yuxiong He. [[Paper]](https://arxiv.org/abs/2307.09782)




## Pre-trained Language Model (PLM)

### 2023
<!-- from https://github.com/horseee/Awesome-Efficient-LLM/tree/main/efficient_plm -->
* [![Star](https://img.shields.io/github/stars/wimh966/outlier_suppression.svg?style=social&label=Star)](https://github.com/wimh966/outlier_suppression) [Outlier Suppression: Pushing the Limit of Low-bit Transformer](https://arxiv.org/abs/2209.13325). Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu. [[Paper]](https://arxiv.org/abs/2209.13325)[[Github]](https://github.com/wimh966/outlier_suppression)
* [![Publish](https://img.shields.io/badge/Conference-ACL'23-blue)]() [Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models](https://aclanthology.org/2023.acl-short.114/). James Oâ€™Neill, Sourav Dutta. [[Paper]](https://aclanthology.org/2023.acl-short.114/)
* [![Publish](https://img.shields.io/badge/Conference-ICML'23-blue)]() [Understanding Int4 Quantization for Language Models: Latency Speedup, Composability, and Failure Cases](https://openreview.net/forum?id=q1WGm3hItW). Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. [[Paper]](https://openreview.net/forum?id=q1WGm3hItW)
* [![Publish](https://img.shields.io/badge/Conference-ACL'23%20Findings-blue)]() [PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models](https://arxiv.org/abs/2306.00014). Zhuocheng Gong, Jiahao Liu, Qifan Wang, Yang Yang, Jingang Wang, Wei Wu, Yunsen Xian, Dongyan Zhao, Rui Yan. [[Paper]](https://arxiv.org/abs/2306.00014)
* [![Publish](https://img.shields.io/badge/Conference-ACL'23%20Findings-blue)]() [Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization](https://aclanthology.org/2023.findings-acl.15.pdf). Chong Yu, Tao Chen, Zhongxue Gan. [[Paper]](https://aclanthology.org/2023.findings-acl.15.pdf)
* [Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing](https://arxiv.org/abs/2306.12929). Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort. [[Paper]](https://arxiv.org/abs/2306.12929)
<!-- from https://github.com/horseee/Awesome-Efficient-LLM/tree/main/efficient_plm -->

* [![Publish](https://img.shields.io/badge/Conference-CVPR'23-blue)]() [NoisyQuant: Noisy Bias-Enhanced Post-Training Activation Quantization for Vision Transformers](https://arxiv.org/abs/2211.16056). Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, Shanghang Zhang. [[Paper]](https://arxiv.org/abs/2211.16056)

* [![Publish](https://img.shields.io/badge/Conference-CVPR'23-blue)]() [Boost Vision Transformer with GPU-Friendly Sparsity and Quantization](https://arxiv.org/abs/2305.10727). Chong Yu, Tao Chen, Zhongxue Gan, Jiayuan Fan. [[Paper]](https://arxiv.org/abs/2305.10727)

* [Q-HyViT: Post-Training Quantization for Hybrid Vision Transformer with Bridge Block Reconstruction](https://arxiv.org/abs/2303.12557). Jemin Lee, Yongin Kwon, Jeman Park, Misun Yu, Hwanjun Song. [[Paper]](https://arxiv.org/abs/2303.12557)

* [LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models](https://arxiv.org/abs/2206.09557). Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee. [[Paper]](https://arxiv.org/abs/2206.09557)


* [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt](https://arxiv.org/abs/2305.11186). Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava. [[Paper]](https://arxiv.org/abs/2305.11186)

<!-- * [![Publish](https://img.shields.io/badge/<leaf tag>-<right tag>-<color>)]() 
[![Star](shields.io_url)](github_url) 
[paper title](paper url). 
some author. 
[[Paper]](paper url)
 [[Github]](github url)-->
